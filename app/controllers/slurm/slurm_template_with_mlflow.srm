#!/bin/bash
#SBATCH -J {experiment_name}   #Nome job
#SBATCH -p {instance_type}     #Fila (partition) a ser utilizada
#SBATCH --account={account}
#SBATCH --nodes=1               #Numero de Nós
#SBATCH --ntasks-per-node=1     #Numero de tarefas por Nó
#SBATCH --ntasks=1              #Numero de tarefas
#SBATCH --exclusive
#SBATCH --exclude atn1b01n16,atn1b01n08,atn1b03n25,atn1b01n19
#SBATCH -o {atena_root}/false_NFS/scripts/{task_id}/job_%j.txt

echo "CUDA_VISIBLE_DEVICES: " $CUDA_VISIBLE_DEVICES
echo $SLURM_JOB_NODELIST
echo "SLURM_JOBID: " $SLURM_JOBID
JOBNAME=$SLURM_JOB_NAME
echo "JOBNAME: " $JOBNAME

# TODO: check if container.sif exists

# deveria haver no .bashrc: esta em miniforge_config.sh
source $HOME/.bashrc

echo "echo proxies:"
echo $HTTP_PROXY
echo $HTTPS_PROXY
echo $http_proxy
echo $https_proxy


singularity run --nv -B {atena_root} \
{sif_path}/teste_sif_py310_miniforge3.sif \
bash -c "export GIT_PYTHON_REFRESH=quiet && export MLFLOW_CONDA_HOME=/miniforge3 && /miniforge3/bin/conda init && source $HOME/.bashrc && export MLFLOW_TRACKING_URI=http://npab1420.petrobras.biz:5000 && mlflow run --experiment-name {experiment_name}_{task_id} {project_root}"
